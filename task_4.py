# -*- coding: utf-8 -*-
"""Task 4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B7iuQfZpBZ3B2egttMPcXf1lz_SzQiAl

# ✈️ Sentiment Analysis on US Airline Tweets
This project performs sentiment analysis using both **VADER** (rule-based) and **RoBERTa** (transformer-based) models.

We use the publicly available **Twitter US Airline Sentiment Dataset** to:
- Preprocess and clean tweet data
- Apply VADER sentiment scoring
- Apply RoBERTa sentiment classification
- Compare model accuracy
- Extract business insights from sentiment trends

## 1 Import Required Libraries
"""

!pip install nrclex

from nrclex import NRCLex

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from transformers import pipeline
from wordcloud import WordCloud
from nrclex import NRCLex

nltk.download('vader_lexicon')
sns.set(style='whitegrid')



"""## 2 Load the Dataset"""

from google.colab import files
uploaded = files.upload()

# Load the uploaded CSV
df = pd.read_csv('Tweets.csv')

# Select relevant columns
df = df[['text', 'airline_sentiment', 'airline', 'negativereason']]

# Drop rows with missing text
df.dropna(subset=['text'], inplace=True)

# Reset index
df.reset_index(drop=True, inplace=True)

# Display top 5 rows
df.head()

"""## 3 Clean the Text"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+", "", text)  # remove URLs
    text = re.sub(r"@\w+", "", text)            # remove mentions
    text = re.sub(r"#\w+", "", text)            # remove hashtags
    text = re.sub(r"[^a-z\s]", "", text)        # remove special characters
    text = re.sub(r"\s+", " ", text).strip()    # normalize whitespace
    return text

df['clean_text'] = df['text'].apply(clean_text)
df[['text', 'clean_text']].head()

"""## 5 Sentiment Distribution"""

plt.figure(figsize=(6, 4))
sns.countplot(x='airline_sentiment', data=df, palette='Set2')
plt.title('Sentiment Distribution')
plt.xlabel('Sentiment')
plt.ylabel('Tweet Count')
plt.show()

"""## 6 Sentiment by Airline"""

plt.figure(figsize=(10, 6))
sns.countplot(x='airline', hue='airline_sentiment', data=df, palette='pastel')
plt.title('Sentiment by Airline')
plt.xlabel('Airline')
plt.ylabel('Number of Tweets')
plt.xticks(rotation=45)
plt.legend(title='Sentiment')
plt.show()

plt.figure(figsize=(10, 5))
df[df['airline_sentiment'] == 'negative']['negativereason'].value_counts().plot(kind='bar', color='tomato')
plt.title('Most Common Negative Reasons')
plt.xlabel('Reason')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

"""## 7 Word Clouds by Sentiment"""

positive_text = ' '.join(df[df['airline_sentiment'] == 'positive']['clean_text'])
negative_text = ' '.join(df[df['airline_sentiment'] == 'negative']['clean_text'])
neutral_text = ' '.join(df[df['airline_sentiment'] == 'neutral']['clean_text'])

plt.figure(figsize=(16, 10))

plt.subplot(1, 3, 1)
plt.imshow(WordCloud(width=500, height=400, background_color='white').generate(positive_text))
plt.title("Positive Tweets")
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(WordCloud(width=500, height=400, background_color='white').generate(neutral_text))
plt.title("Neutral Tweets")
plt.axis('off')

plt.subplot(1, 3, 3)
plt.imshow(WordCloud(width=500, height=400, background_color='white').generate(negative_text))
plt.title("Negative Tweets")
plt.axis('off')

plt.tight_layout()
plt.show()

"""## 8 Initialize the VADER Sentiment Analyzer"""

# Initialize VADER
sid = SentimentIntensityAnalyzer()

"""## 8.1 Get Sentiment Scores for Each Tweet"""

# Apply VADER on clean text
df['vader_scores'] = df['clean_text'].apply(lambda x: sid.polarity_scores(x)['compound'])

# Map compound score to sentiment
def get_vader_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['vader_sentiment'] = df['vader_scores'].apply(get_vader_sentiment)
df[['clean_text', 'vader_scores', 'vader_sentiment']].head()

"""## 8.2 Compare VADER with Original Labels"""

from sklearn.metrics import classification_report, accuracy_score

print("VADER vs Original Sentiment Accuracy:")
print(accuracy_score(df['airline_sentiment'], df['vader_sentiment']))

print("\nClassification Report:")
print(classification_report(df['airline_sentiment'], df['vader_sentiment']))

"""## 9 Load the Pretrained RoBERTa Model"""

# Load sentiment analysis pipeline using RoBERTa
roberta_classifier = pipeline("sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment")

"""## 9.1 Apply RoBERTa to Tweets"""

# Apply to a subset (can increase if needed)
sample_df = df.head(500).copy()

# Get predictions
sample_df['roberta_result'] = sample_df['clean_text'].apply(lambda x: roberta_classifier(x)[0])
sample_df['roberta_label'] = sample_df['roberta_result'].apply(lambda x: x['label'].lower())
sample_df['roberta_score'] = sample_df['roberta_result'].apply(lambda x: x['score'])

sample_df[['clean_text', 'roberta_label', 'roberta_score']].head()

"""## 9.2 Evaluate RoBERTa vs Original Sentiment"""

print("RoBERTa vs Original Sentiment Accuracy:")
print(accuracy_score(sample_df['airline_sentiment'], sample_df['roberta_label']))

print("\nClassification Report:")
print(classification_report(sample_df['airline_sentiment'], sample_df['roberta_label']))

"""## 10 Apply NRCLex for Emotion Extraction"""

import nltk
nltk.download('punkt_tab')
def get_emotions(text):
    emotion = NRCLex(text)
    return emotion.raw_emotion_scores

df['emotions'] = df['clean_text'].apply(get_emotions)
df['emotions'].head()

"""## 10.1 Convert Emotion Dictionary to DataFrame"""

# Convert list of emotion dicts to a DataFrame
emotion_df = pd.json_normalize(df['emotions'])

# Replace NaN with 0
emotion_df.fillna(0, inplace=True)

# Add to original DataFrame
df = pd.concat([df, emotion_df], axis=1)
df.head()

"""## 10.2 Plot Overall Emotion Distribution"""

emotion_totals = emotion_df.sum().sort_values(ascending=False)

plt.figure(figsize=(10,5))
sns.barplot(x=emotion_totals.index, y=emotion_totals.values, palette='Spectral')
plt.title("Overall Emotion Distribution in Tweets")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

"""## 11 Extract Most Common Keywords"""

from sklearn.feature_extraction.text import CountVectorizer

# Initialize CountVectorizer
cv = CountVectorizer(stop_words='english', max_features=30)
X = cv.fit_transform(df['clean_text'])

# Get top keywords
keywords = pd.DataFrame(cv.get_feature_names_out(), columns=['Keyword'])
keywords['Frequency'] = X.toarray().sum(axis=0)
keywords.sort_values(by='Frequency', ascending=False, inplace=True)

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x='Frequency', y='Keyword', data=keywords, palette='viridis')
plt.title("Top Keywords in Tweets")
plt.xlabel("Frequency")
plt.ylabel("Keyword")
plt.show()

"""## 11.1.1 Vectorize Tweets for LDA"""

from sklearn.feature_extraction.text import CountVectorizer

# Vectorizer for LDA (no max_features to preserve topics)
lda_vectorizer = CountVectorizer(stop_words='english')
X_lda = lda_vectorizer.fit_transform(df['clean_text'])

"""## 11.1.2 Apply LDA to Discover Topics"""

from sklearn.decomposition import LatentDirichletAllocation

# Train LDA with 5 topics
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_model.fit(X_lda)

"""## 11.1.3 Display Top Keywords per Topic"""

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"\n🧠 Topic #{topic_idx + 1}:")
        print(" | ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

display_topics(lda_model, lda_vectorizer.get_feature_names_out(), 10)

"""## 11.1.4 Assign Dominant Topic to Each Tweet (Optional)"""

# Assign dominant topic to each tweet
df['lda_topic'] = lda_model.transform(X_lda).argmax(axis=1)
df[['clean_text', 'lda_topic']].head()

vader = SentimentIntensityAnalyzer()
df['vader_score'] = df['text'].apply(lambda x: vader.polarity_scores(x)['compound'])

def classify_vader(compound):
    if compound >= 0.05:
        return 'positive'
    elif compound <= -0.05:
        return 'negative'
    else:
        return 'neutral'

df['vader_sentiment'] = df['vader_score'].apply(classify_vader)

roberta_pipeline = pipeline('sentiment-analysis')
df_sample = df.sample(1000, random_state=42).copy()
roberta_results = roberta_pipeline(df_sample['text'].tolist(), truncation=True)
df_sample['roberta_label'] = [res['label'].lower() for res in roberta_results]

plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.countplot(x='vader_sentiment', data=df)
plt.title('VADER Sentiment Distribution')

plt.subplot(1, 2, 2)
sns.countplot(x='roberta_label', data=df_sample)
plt.title('RoBERTa Sentiment Distribution')
plt.tight_layout()
plt.show()

vader_accuracy = (df['vader_sentiment'] == df['airline_sentiment']).mean()
roberta_accuracy = (df_sample['roberta_label'] == df_sample['airline_sentiment']).mean()
print(f"VADER Accuracy: {vader_accuracy:.2%}")
print(f"RoBERTa Accuracy: {roberta_accuracy:.2%}")

top_neg_reasons = df[df['airline_sentiment'] == 'negative']['negativereason'].value_counts().head(5)
print("Top 5 reasons for negative sentiment:")
print(top_neg_reasons)

